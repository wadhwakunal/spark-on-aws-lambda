AWSTemplateFormatVersion: '2010-09-09'
Transform: 'AWS::Serverless-2016-10-31'

Parameters:
  ImageUri:
    Description: 'Mandatory: ECR Uri for the image. If not present, then create and push using the sam-imagebuilder.yaml in the github https://github.com/aws-samples/spark-on-aws-lambda '
    Type: String
    Default: ''
  ScriptBucket:
    Type: String
    Description: 'Mandatory: Amazon S3 Bucket name where the spark script is stored. Just the bucket name e.g. bucket1'
  SparkScript:
    Type: String
    Description: 'Mandatory: Amazon S3 key where the spark script resides. Start without the /.eg: script/location/name.py'
  DatabaseName:
    Type: String
    Description: 'Mandatory: Glue Catalog Database where the table resides e.g. bb_datalake_validated'
  TableName:
    Type: String
    Description: 'Mandatory: Glue Catalog output table e.g. bb_markets'
  AthenaWorkgroup:
    Type: String
    Description: 'Mandatory: Athena Workgroup e.g. bb-datalake'
  LambdaFunctionPrefix:
    Description: 'Optional: This is the prefix for the name of the lambda function that will be created. This name must satisfy the pattern ^[a-z0-9-_]{1,64}$'
    Type: String
    Default: Sparkjob
  LambdaTimeout:
    Description: 'Optional: Maximum Lambda invocation runtime in seconds. (min 1 - 900 max)'
    Default: 900
    Type: Number
  LambdaMemory:
    Description: 'Optional: Lambda memory in MB (min 128 - 3008 max).'
    Default: 3008
    Type: Number
  SparkLambdapermissionPolicyArn:
    Description: 'Optional: Arn of the policy that contains the permissions your spark job will need to run successfully'
    Type: String
    Default: ''
  GlueJobName:
    Description: 'Mandatory: Name to be assigned to the Glue Job'
    Type: String
  GlueJobRole:
    Description: 'Mandatory: IAM role to be assigned to the Glue Job'
    Type: String
  GlueScriptBucket:
    Description: 'Mandatory: S3 bucket where PySpark script is placed'
    Type: String
  GlueScriptKey:
    Description: 'Mandatory: S3 key where PySpark script is placed'
    Type: String
  AttachToVpc:
    Type: String
    Description: 'Mandatory: Set True or False to imply VPC Connectivity'
    Default: False
    AllowedValues:
      - True
      - False
  SecurityGroupIds:
    Description: 'Optional: One or more SecurityGroup IDs corresponding to the SecurityGroup that should be applied to the Lambda function. (e.g. sg1,sg2,sg3).Only used if AttachToVpc is True'
    Type: 'CommaDelimitedList'
    Default: ''
  SubnetIds:
    Description: 'Optional: One or more Subnet IDs corresponding to the Subnet that the Lambda function can use to access you data source. (e.g. subnet1,subnet2).Only used if AttachToVpc is True'
    Type: 'CommaDelimitedList'
    Default: ''
  
  Command:
    Description: 'Optional: Command override for the image. This is not required'
    Type: 'CommaDelimitedList'
    Default: "/var/task/sparkLambdaHandler.lambda_handler"
  EntryPoint:
    Description: 'Optional: Entry Point override for the image'
    Type: 'CommaDelimitedList'
    Default: ''
  WorkingDirectory:
    Description: 'Optional: Command override for the image'
    Type: 'String'
    Default: ''

Conditions:
  NeedsVPC:  !Equals [ !Ref AttachToVpc, 'True' ]
  HasAdditionalPolicy: !Not  [ !Equals [ '', !Ref SparkLambdapermissionPolicyArn ] ]
  NeedsImageBuild: !Not [ !Equals [ !Ref ImageUri, '' ]]
  HasEntryPoint: !Not  [ !Equals [ '' , !Join [ ',',  !Ref EntryPoint ] ] ]
  HasWorkingDirectory: !Not  [ !Equals [ '', !Ref WorkingDirectory ] ]

Resources:
  Resources:
  PySparkGlueJob:
    Type: "AWS::Glue::Job"
    Properties:
      Role: !Sub arn:aws:iam::${AWS::AccountId}:role/${GlueJobRole}
      Name: !Ref GlueJobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Sub "s3://${GlueScriptBucket}/${GlueScriptKey}"
      }
      DefaultArguments: {
          "--TempDir": !Sub "s3://${GlueScriptBucket}/tmp",
          "--enable-continuous-cloudwatch-log": "true",
          "--enable-continuous-log-filter": "false",
          "--enable-glue-datacatalog": "true",
          "--job-bookmark-option": "job-bookmark-disable",
          "--datalake-formats": "delta",
          "--INPUT_PATHS": "unprocessed_files",
          "--ERROR_FILE_BUCKET": "error_file_bucket",
          "--ERROR_FILE_KEY": "error_file_key"
      }
      GlueVersion: "3.0"
      MaxRetries: 0
      Description: "Glue job to process data in case Spark Lambda is not able to handle the data volume"
      MaxCapacity: 2

  SparkLambda:
    Type: 'AWS::Serverless::Function'
    Properties:
      PackageType: Image
      FunctionName: !Sub '${LambdaFunctionPrefix}-${AWS::StackName}'
      ImageUri: !Ref ImageUri
      ImageConfig:
        Command: !Ref Command
        EntryPoint:
          !If
            - HasEntryPoint
            - !Ref EntryPoint
            - !Ref "AWS::NoValue"
        WorkingDirectory:
          !If
            - HasWorkingDirectory
            - !Ref WorkingDirectory
            - !Ref "AWS::NoValue"
      Description: "Lambda to run spark containers"
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Role: !Sub "arn:aws:iam::${AWS::AccountId}:role/service-role/datalake-lambda-s3-permission"
      ReservedConcurrentExecutions: 1
      VpcConfig:
        !If
          - NeedsVPC
          -
            SecurityGroupIds: !Ref SecurityGroupIds
            SubnetIds: !Ref SubnetIds
          - !Ref "AWS::NoValue"
      Environment:
        Variables:
          SCRIPT_BUCKET: !Ref ScriptBucket
          SPARK_SCRIPT: !Ref SparkScript
          DATABASE_NAME: !Ref DatabaseName
          TABLE_NAME: !Ref TableName
          ATHENA_WORKGROUP: !Ref AthenaWorkgroup
  
  SparkLambdaInvokeConfig:
    Type: 'AWS::Lambda::EventInvokeConfig'
    Properties:
      FunctionName: !Ref SparkLambda
      MaximumRetryAttempts: 0
      Qualifier: "$LATEST"