AWSTemplateFormatVersion: '2010-09-09'
Transform: 'AWS::Serverless-2016-10-31'

Parameters:
  FilesCollectorLambdaFunctionPrefix:
    Description: 'Optional: This is the prefix for the name of the lambda function that will be created. This name must satisfy the pattern ^[a-z0-9-_]{1,64}$'
    Type: String
    Default: FilesCollector
  FilesCollectorLambdaTimeout:
    Description: 'Optional: Maximum Lambda invocation runtime in seconds. (min 1 - 900 max)'
    Default: 60
    Type: Number
  FilesCollectorLambdaMemory:
    Description: 'Optional: Lambda memory in MB (min 128 - 3008 max).'
    Default: 128
    Type: Number
  FilesCollectorMetadataBucketName:
    Type: String
    Description: 'Mandatory: Bucket where the metadata will be persisted e.g. bb-datalake'
  FilesCollectorMetadataKeyName:
    Type: String
    Description: 'Mandatory: Key where the metadata will be persisted e.g. metadata/bb_market_delta'
  FilesCollectorSourceBucketName:
    Type: String
    Description: 'Mandatory: Source bucket where raw files will land e.g. bb-market-prod-kinesis-datastreams-dynamo'
  FilesCollectorNotificationsSaveMode:
    Type: String
    Description: 'Optional: Notifications can be inserted or appended So only two values allowed: Insert/Append'
    Default: Append
  FilesMergerLambdaFunctionPrefix:
    Description: 'Optional: This is the prefix for the name of the lambda function that will be created. This name must satisfy the pattern ^[a-z0-9-_]{1,64}$'
    Type: String
    Default: FilesMerger
  FilesMergerLambdaTimeout:
    Description: 'Optional: Maximum Lambda invocation runtime in seconds. (min 1 - 900 max)'
    Default: 120
    Type: Number
  FilesMergerLambdaMemory:
    Description: 'Optional: Lambda memory in MB (min 128 - 3008 max).'
    Default: 128
    Type: Number
  FilesMergerSleepTime:
    Type: Number
    Description: 'Mandatory: Sleep time in seconds to recursively check for flag file e.g. 5'
  FilesMergerSourceBucketName:
    Type: String
    Description: 'Mandatory: Source bucket where unprocessed_file will be placed e.g. bb-datalake'
  FilesMergerTriggerFile:
    Type: String
    Description: 'Optional: File to trigger Merger Lambda e.g. unprocessed_tmp'
    Default: unprocessed_tmp
  FilesMergerNotificationsSaveMode:
    Type: String
    Description: 'Optional: Notifications can be inserted or appended So only two values allowed: Insert/Append'
    Default: Append
  SparkLambdaImageUri:
    Description: 'Mandatory: ECR Uri for the image. If not present, then create and push using the sam-imagebuilder.yaml in the github https://github.com/aws-samples/spark-on-aws-lambda '
    Type: String
    Default: ''
  SparkLambdaScriptBucket:
    Type: String
    Description: 'Mandatory: Amazon S3 Bucket name where the spark script is stored. Just the bucket name e.g. bucket1'
  SparkLambdaSparkScript:
    Type: String
    Description: 'Mandatory: Amazon S3 key where the spark script resides. Start without the /.eg: script/location/name.py'
  SparkLambdaDatabaseName:
    Type: String
    Description: 'Mandatory: Glue Catalog Database where the table resides e.g. bb_datalake_validated'
  SparkLambdaTableName:
    Type: String
    Description: 'Mandatory: Glue Catalog output table e.g. bb_markets'
  SparkLambdaAthenaWorkgroup:
    Type: String
    Description: 'Mandatory: Athena Workgroup e.g. bb-datalake'
  SparkLambdaFunctionPrefix:
    Description: 'Optional: This is the prefix for the name of the lambda function that will be created. This name must satisfy the pattern ^[a-z0-9-_]{1,64}$'
    Type: String
    Default: SparkJob
  SparkLambdaTimeout:
    Description: 'Optional: Maximum Lambda invocation runtime in seconds. (min 1 - 900 max)'
    Default: 900
    Type: Number
  SparkLambdaMemory:
    Description: 'Optional: Lambda memory in MB (min 128 - 3008 max).'
    Default: 3008
    Type: Number
  SparkLambdapermissionPolicyArn:
    Description: 'Optional: Arn of the policy that contains the permissions your spark job will need to run successfully'
    Type: String
    Default: ''
  SparkLambdaAttachToVpc:
    Type: String
    Description: 'Mandatory: Set True or False to imply VPC Connectivity'
    Default: False
    AllowedValues:
      - True
      - False
  SparkLambdaSecurityGroupIds:
    Description: 'Optional: One or more SecurityGroup IDs corresponding to the SecurityGroup that should be applied to the Lambda function. (e.g. sg1,sg2,sg3).Only used if AttachToVpc is True'
    Type: 'CommaDelimitedList'
    Default: ''
  SparkLambdaSubnetIds:
    Description: 'Optional: One or more Subnet IDs corresponding to the Subnet that the Lambda function can use to access you data source. (e.g. subnet1,subnet2).Only used if AttachToVpc is True'
    Type: 'CommaDelimitedList'
    Default: ''
  SparkLambdaTriggerFile:
    Type: String
    Description: 'Optional: File to trigger Spark Lambda e.g. unprocessed_file'
    Default: unprocessed_file
  SparkLambdaNotificationsSaveMode:
    Type: String
    Description: 'Optional: Notifications can be inserted or appended So only two values allowed: Insert/Append'
    Default: Append

Command:
    Description: 'Optional: Command override for the image. This is not required'
    Type: 'CommaDelimitedList'
    Default: "/var/task/sparkLambdaHandler.lambda_handler"
  EntryPoint:
    Description: 'Optional: Entry Point override for the image'
    Type: 'CommaDelimitedList'
    Default: ''
  WorkingDirectory:
    Description: 'Optional: Command override for the image'
    Type: 'String'
    Default: ''

Conditions:
  NeedsVPC:  !Equals [ !Ref SparkLambdaAttachToVpc, 'True' ]
  HasAdditionalPolicy: !Not  [ !Equals [ '', !Ref SparkLambdapermissionPolicyArn ] ]
  NeedsImageBuild: !Not [ !Equals [ !Ref SparkLambdaImageUri, '' ]]
  HasEntryPoint: !Not  [ !Equals [ '' , !Join [ ',',  !Ref EntryPoint ] ] ]
  HasWorkingDirectory: !Not  [ !Equals [ '', !Ref WorkingDirectory ] ]
  
Resources:
  FilesCollectorLambda:
    Type: 'AWS::Serverless::Function'
    Properties:
      FunctionName: !Sub '${FilesCollectorLambdaFunctionPrefix}-${AWS::StackName}'
      Handler: index.lambda_handler
      InlineCode: |
          import json
          import logging
          import boto3
          import botocore
          import os

          # Set up logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")

          #Get environment variables
          metadata_bucket_name = os.environ["Metadata_Bucket_Name"]
          metadata_key_name = os.environ["Metadata_Key_Name"]

          def lambda_handler(event, context):
            s3_object = getS3Object(event)
            writeToS3(s3_object)

          def getS3Object(event):
            logger.info("Inside function getS3Object")
            bucket_name = event["Records"][0]["s3"]["bucket"]["name"]
            object_key = event["Records"][0]["s3"]["object"]["key"]
    
            s3_object = "s3a://" + bucket_name + "/" + object_key
            logger.info(f"S3 Object Name: {s3_object}")
    
            return s3_object

          def writeToS3(s3_object):
            logger.info("Inside function writeToS3")
            encoded_string = s3_object.encode("utf-8")
        
            file_name = "unprocessed"
            s3_path = metadata_key_name + "/" + file_name + "_tmp"
            s3_path_flag = metadata_key_name + "/" + file_name + "_flag"

            s3 = boto3.resource("s3")
        
            try:
              body_flag_file = s3.Object(metadata_bucket_name, s3_path_flag).get()['Body'].read().decode('utf-8')
              if int(body_flag_file) == 0:
                read_content_and_push_file(metadata_bucket_name, s3_path, encoded_string)
                flag_int = int(body_flag_file) + 1
                flag_string = str(flag_int)
                logger.info(f"Content successfully written to file {s3_path}")
              else:
                new_file_name = (s3_path.replace("_tmp","")).replace(file_name,"")+f"tmp_{body_flag_file}_{file_name}"
                read_content_and_push_file(metadata_bucket_name, new_file_name, encoded_string)
                flag_int = int(body_flag_file) + 1
                if flag_int > 10:
                  flag_string = "0"
                else:
                  flag_string = str(flag_int)
                logger.info(f"Content successfully written to file {new_file_name}")
              s3.Bucket(metadata_bucket_name).put_object(Key=s3_path_flag, Body=flag_string.encode("utf-8"))
              logger.info(f"Flag successfully written to file {s3_path_flag}")
            except botocore.exceptions.ClientError as e:
              if e.response["Error"]["Code"] == "NoSuchKey":
                s3.Bucket(metadata_bucket_name).put_object(Key=s3_path, Body=encoded_string)
                s3.Bucket(metadata_bucket_name).put_object(Key=s3_path_flag, Body="1".encode("utf-8"))
                logger.info(f"Content successfully written to file {s3_path}")
                logger.info(f"Flag successfully written to file {s3_path_flag}")
              else:
                logger.info(f"Error: {e.response['Error']['Code']}")

          def read_content_and_push_file(bucket_name, file_name, new_content) -> None:
            s3 = boto3.resource("s3")
            try:
              content = s3.Object(bucket_name, file_name).get()['Body'].read().decode('utf-8')
              content = content.strip() + "\n" + new_content.decode('utf-8')
              encoded_content = content.encode('utf-8')
              s3.Bucket(bucket_name).put_object(Key=file_name, Body=encoded_content)
            except Exception as e :
              s3.Bucket(bucket_name).put_object(Key=file_name, Body=new_content)
      Runtime: python3.10
      Description: A function that collects all the file names from S3Trigger.
      Role: !Sub "arn:aws:iam::${AWS::AccountId}:role/service-role/datalake-lambda-s3-permission"
      Timeout: !Ref FilesCollectorLambdaTimeout
      MemorySize: !Ref FilesCollectorLambdaMemory
      ReservedConcurrentExecutions: 1
      Environment:
        Variables:
          Metadata_Bucket_Name: !Ref FilesCollectorMetadataBucketName
          Metadata_Key_Name: !Ref FilesCollectorMetadataKeyName
  
  FilesCollectorLambdaInvokeConfig:
    Type: 'AWS::Lambda::EventInvokeConfig'
    DependsOn: FilesCollectorLambda
    Properties:
      FunctionName: !Ref FilesCollectorLambda
      MaximumRetryAttempts: 0
      Qualifier: "$LATEST"

  FilesCollectorLambdaInvokePermission:
    Type: 'AWS::Lambda::Permission'
    Properties:
      FunctionName: !GetAtt FilesCollectorLambda.Arn
      Action: 'lambda:InvokeFunction'
      Principal: s3.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
      SourceArn: !Sub 'arn:aws:s3:::${FilesCollectorSourceBucketName}'

  FilesCollectorS3TriggerLambdaRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Path: /
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetBucketNotification'
                  - 's3:PutBucketNotification'
                Resource: !Sub 'arn:aws:s3:::${FilesCollectorSourceBucketName}'
              - Effect: Allow
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: 'arn:aws:logs:*:*:*'
  
  FilesCollectorS3TriggerLambda:
    Type: 'AWS::Lambda::Function'
    DependsOn: FilesCollectorLambdaInvokeConfig
    Properties:
      FunctionName: !Sub 'FilesCollectorS3TriggerLambda-${AWS::StackName}'
      Handler: index.lambda_handler
      Code:
        ZipFile: |

          import json
          import logging
          import boto3
          import cfnresponse

          # Set up logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")

          def lambda_handler(event, context):
            logger.info("Inside function to add S3 Trigger")
            logger.info("Received event: " + json.dumps(event, indent=2))

            source_bucket_name = event['ResourceProperties']["SourceBucketName"]
            target_lambda_arn = event['ResourceProperties']['TargetLambdaArn']
            notifications_save_mode = event['ResourceProperties']['NotificationsSaveMode']
            
            try:
              lambda_configurations = []
              if notifications_save_mode.lower() == "insert":
                lambda_configurations.append({'LambdaFunctionArn': target_lambda_arn, 'Events': ['s3:ObjectCreated:*']})
              elif notifications_save_mode.lower() == "append":
                s3_client = boto3.client('s3')
                existing_configurations = s3_client.get_bucket_notification_configuration(Bucket=source_bucket_name)
                for config in existing_configurations['LambdaFunctionConfigurations']:
                  config_dict = {}
                  config_dict['LambdaFunctionArn'] = config['LambdaFunctionArn']
                  config_dict['Events'] = config['Events']
                  lambda_configurations.append(config_dict)
                lambda_configurations.append({'LambdaFunctionArn': target_lambda_arn, 'Events': ['s3:ObjectCreated:*']})
              else:
                raise Exception("Notifications save mode is invalid. Permissible values: Append/Insert.")
              logger.info(f"Printing S3 Trigger config: {lambda_configurations}")
              s3 = boto3.resource('s3')  
              bucket_notification = s3.BucketNotification(source_bucket_name)
              response = bucket_notification.put(
                NotificationConfiguration={
                  'LambdaFunctionConfigurations': lambda_configurations
              })
              responseData={'source_bucket_name':source_bucket_name}
              responseStatus = 'SUCCESS'
              logger.info(f"S3 Trigger successfully added for {source_bucket_name}")
            except Exception as e:
              responseStatus = 'FAILED'
              responseData = {'Failure': 'Something bad happened.'}
              logger.info(f"S3 Trigger failed for {source_bucket_name}: {e}")
              raise e
            cfnresponse.send(event, context, responseStatus, responseData, "CustomResourcePhysicalID")
      Runtime: python3.10
      Description: A function that collects all the file names from S3Trigger.
      Role: !GetAtt FilesCollectorS3TriggerLambdaRole.Arn
      Timeout: 50

  FilesCollectorInvokeS3TriggerLambda:
    Type: 'Custom::InvokeS3TriggerLambda'
    DependsOn: FilesCollectorLambdaInvokePermission
    Properties:
      ServiceToken: !GetAtt FilesCollectorS3TriggerLambda.Arn
      TargetLambdaArn: !GetAtt FilesCollectorLambda.Arn
      SourceBucketName: !Ref FilesCollectorSourceBucketName
      NotificationsSaveMode: !Ref FilesCollectorNotificationsSaveMode

  FilesMergerLambda:
    Type: 'AWS::Serverless::Function'
    Properties:
      FunctionName: !Sub '${FilesMergerLambdaFunctionPrefix}-${AWS::StackName}'
      Handler: index.lambda_handler
      InlineCode: |
          import json
          import boto3
          import botocore
          import logging
          import time
          import os

          # Set up logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")

          def lambda_handler(event, context):
            bucket_name = event["Records"][0]["s3"]["bucket"]["name"]
            object_key = event["Records"][0]["s3"]["object"]["key"]
            flag_file = object_key.replace("_tmp","_flag") 
    
            sleep_time_secs = os.environ['Sleep_Time']

            add_delay_to_execution(bucket_name,flag_file)
            merge_files_content(bucket_name,object_key)

          def add_delay_to_execution(s3_bucket_script: str,object_key: str) -> None:
            try:
              logger.info('Inside function add_delay_to_execution')
              s3 = boto3.resource("s3")
              last_modified_date = s3.Object(s3_bucket_script, object_key).last_modified
              current_modified_date = ""
              i = 1
              while current_modified_date != last_modified_date:
                time.sleep(sleep_time_secs)
                current_modified_date = last_modified_date
                last_modified_date = s3.Object(s3_bucket_script, object_key).last_modified
                logger.info(f'Iteration : {i}, last_modified_date: {last_modified_date}, current_modified_date: {current_modified_date}')
                i = i + 1
            except botocore.exceptions.ClientError as e:
              logger.error(f"Boto Error: {e.response['Error']['Code']}")
            except Exception as e :
              logger.error(f"Error: {e}")
            else:
              logger.info('Successfully added delay to the execution')

          def merge_files_content(bucket_name: str,object_key: str) -> None:
            try:
              logger.info('Inside function merge_files_content')
              s3 = boto3.resource("s3")
              required_objects = s3.Bucket(bucket_name).objects.filter(Prefix=object_key.replace("unprocessed_tmp",""))
              content = ""
              try:
                logger.info(f'Reading {object_key.replace("_tmp","_file")} if it exists')
                content = s3.Object(bucket_name,object_key.replace("_tmp","_file")).get()['Body'].read().decode('utf-8') + "\n"
                logger.info(f'{object_key.replace("_tmp","_file")} exists, Content: {content}')
              except botocore.exceptions.ClientError as e:
                logger.info(f'{object_key.replace("_tmp","_file")} does not exist')
              for object in required_objects:
                if "tmp" in object.key:
                  logger.info(f'Merging file: {object.key}')
                  content = content + object.get()['Body'].read().decode('utf-8') + "\n"
              content = content.strip()
              logger.info(f'Merged content: {content}')
              logger.info(f'Writing merged content to file: {object_key.replace("_tmp","_file")}')
              s3.Bucket(bucket_name).put_object(Key=object_key.replace("_tmp","_file"), Body=content.encode('utf-8'))
              logger.info('Deleting tmp files')
              for object in required_objects:
                if "tmp" in object.key:
                  logger.info(f'Deleting file: {object.key}')
                  object.delete()
              logger.info(f'Deleting file: {object_key.replace("_tmp","_flag")}')
              s3.Object(bucket_name, object_key.replace("_tmp","_flag")).delete()
            except botocore.exceptions.ClientError as e:
              logger.error(f"Boto Error: {e.response['Error']['Code']}")
            except Exception as e :
              logger.error(f"Error: {e}")
            else:
              logger.info(f'Successfully merged files content and written to {object_key.replace("_tmp","_file")}')
      Runtime: python3.10
      Description: A function that merges all the files containing s3 file names into a single file(unprocessed_file).
      Role: !Sub "arn:aws:iam::${AWS::AccountId}:role/service-role/datalake-lambda-s3-permission"
      Timeout: !Ref FilesMergerLambdaTimeout
      MemorySize: !Ref FilesMergerLambdaMemory
      ReservedConcurrentExecutions: 1
      Environment:
        Variables:
          Sleep_Time: !Ref FilesMergerSleepTime
  
  FilesMergerLambdaInvokeConfig:
    Type: 'AWS::Lambda::EventInvokeConfig'
    DependsOn: FilesMergerLambda
    Properties:
      FunctionName: !Ref FilesMergerLambda
      MaximumRetryAttempts: 0
      Qualifier: "$LATEST"

  FilesMergerLambdaInvokePermission:
    Type: 'AWS::Lambda::Permission'
    Properties:
      FunctionName: !GetAtt FilesMergerLambda.Arn
      Action: 'lambda:InvokeFunction'
      Principal: s3.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
      SourceArn: !Sub 'arn:aws:s3:::${FilesMergerSourceBucketName}'

  FilesMergerS3TriggerLambdaRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Path: /
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetBucketNotification'
                  - 's3:PutBucketNotification'
                Resource: !Sub 'arn:aws:s3:::${FilesMergerSourceBucketName}'
              - Effect: Allow
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: 'arn:aws:logs:*:*:*'
  
  FilesMergerS3TriggerLambda:
    Type: 'AWS::Lambda::Function'
    DependsOn: FilesMergerLambdaInvokeConfig
    Properties:
      FunctionName: !Sub 'FilesMergerS3TriggerLambda-${AWS::StackName}'
      Handler: index.lambda_handler
      Code:
        ZipFile: |

          import json
          import logging
          import boto3
          import cfnresponse

          # Set up logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")

          def lambda_handler(event, context):
            logger.info("Inside function to add S3 Trigger")
            logger.info("Received event: " + json.dumps(event, indent=2))

            source_bucket_name = event['ResourceProperties']["SourceBucketName"]
            target_lambda_arn = event['ResourceProperties']['TargetLambdaArn']
            notifications_save_mode = event['ResourceProperties']['NotificationsSaveMode']
            prefix_pattern = event['ResourceProperties']['FilesMergerTriggerFilePattern']
            
            try:
              lambda_configurations = []
              if notifications_save_mode.lower() == "insert":
                lambda_configurations.append({'LambdaFunctionArn': target_lambda_arn, 'Events': ['s3:ObjectCreated:*']})
              elif notifications_save_mode.lower() == "append":
                s3_client = boto3.client('s3')
                existing_configurations = s3_client.get_bucket_notification_configuration(Bucket=source_bucket_name)
                for config in existing_configurations['LambdaFunctionConfigurations']:
                  config_dict = {}
                  config_dict['LambdaFunctionArn'] = config['LambdaFunctionArn']
                  config_dict['Events'] = config['Events']
                  config_dict['Filter'] = config['Filter']
                  lambda_configurations.append(config_dict)
                lambda_configurations.append({'LambdaFunctionArn': target_lambda_arn, 'Events': ['s3:ObjectCreated:*'], 'Filter': {'Key': {'FilterRules': [{'Name': 'Prefix', 'Value': f'{prefix_pattern}'}]}}})
              else:
                raise Exception("Notifications save mode is invalid. Permissible values: Append/Insert.")
              logger.info(f"Printing S3 Trigger config: {lambda_configurations}")
              s3 = boto3.resource('s3')  
              bucket_notification = s3.BucketNotification(source_bucket_name)
              response = bucket_notification.put(
                NotificationConfiguration={
                  'LambdaFunctionConfigurations': lambda_configurations
              })
              responseData={'source_bucket_name':source_bucket_name}
              responseStatus = 'SUCCESS'
              logger.info(f"S3 Trigger successfully added for {source_bucket_name}")
            except Exception as e:
              responseStatus = 'FAILED'
              responseData = {'Failure': 'Something bad happened.'}
              logger.info(f"S3 Trigger failed for {source_bucket_name}: {e}")
              raise e
            cfnresponse.send(event, context, responseStatus, responseData, "CustomResourcePhysicalID")
      Runtime: python3.10
      Description: A function that merges all the file names from S3 tmp files.
      Role: !GetAtt FilesMergerS3TriggerLambdaRole.Arn
      Timeout: 50

  FilesMergerInvokeS3TriggerLambda:
    Type: 'Custom::InvokeFilesMergerS3TriggerLambda'
    DependsOn: FilesMergerLambdaInvokePermission
    Properties:
      ServiceToken: !GetAtt FilesMergerS3TriggerLambda.Arn
      TargetLambdaArn: !GetAtt FilesMergerLambda.Arn
      SourceBucketName: !Ref FilesMergerSourceBucketName
      NotificationsSaveMode: !Ref FilesMergerNotificationsSaveMode
      FilesMergerTriggerFilePattern: !Sub '${FilesCollectorMetadataKeyName}/${FilesMergerTriggerFile}'

  SparkLambda:
    Type: 'AWS::Serverless::Function'
    Properties:
      PackageType: Image
      FunctionName: !Sub '${SparkLambdaFunctionPrefix}-${AWS::StackName}'
      ImageUri: !Ref SparkLambdaImageUri
      ImageConfig:
        Command: !Ref Command
        EntryPoint:
          !If
            - HasEntryPoint
            - !Ref EntryPoint
            - !Ref "AWS::NoValue"
        WorkingDirectory:
          !If
            - HasWorkingDirectory
            - !Ref WorkingDirectory
            - !Ref "AWS::NoValue"
      Description: "Lambda to run spark containers"
      Timeout: !Ref SparkLambdaTimeout
      MemorySize: !Ref SparkLambdaMemory
      Role: !Sub "arn:aws:iam::${AWS::AccountId}:role/service-role/datalake-lambda-s3-permission"
      ReservedConcurrentExecutions: 1
      VpcConfig:
        !If
          - NeedsVPC
          -
            SecurityGroupIds: !Ref SparkLambdaSecurityGroupIds
            SubnetIds: !Ref SparkLambdaSubnetIds
          - !Ref "AWS::NoValue"
      Environment:
        Variables:
          SCRIPT_BUCKET: !Ref SparkLambdaScriptBucket
          SPARK_SCRIPT: !Ref SparkLambdaSparkScript
          DATABASE_NAME: !Ref SparkLambdaDatabaseName
          TABLE_NAME: !Ref SparkLambdaTableName
          ATHENA_WORKGROUP: !Ref SparkLambdaAthenaWorkgroup
  
  SparkLambdaInvokeConfig:
    Type: 'AWS::Lambda::EventInvokeConfig'
    Properties:
      FunctionName: !Ref SparkLambda
      MaximumRetryAttempts: 0
      Qualifier: "$LATEST"

  SparkLambdaInvokePermission:
    Type: 'AWS::Lambda::Permission'
    Properties:
      FunctionName: !GetAtt SparkLambda.Arn
      Action: 'lambda:InvokeFunction'
      Principal: s3.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
      SourceArn: !Sub 'arn:aws:s3:::${FilesMergerSourceBucketName}'

  SparkLambdaS3TriggerLambdaRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Path: /
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetBucketNotification'
                  - 's3:PutBucketNotification'
                Resource: !Sub 'arn:aws:s3:::${FilesMergerSourceBucketName}'
              - Effect: Allow
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: 'arn:aws:logs:*:*:*'
  
  SparkLambdaS3TriggerLambda:
    Type: 'AWS::Lambda::Function'
    DependsOn: SparkLambdaInvokeConfig
    Properties:
      FunctionName: !Sub 'SparkLambdaS3TriggerLambda-${AWS::StackName}'
      Handler: index.lambda_handler
      Code:
        ZipFile: |

          import json
          import logging
          import boto3
          import cfnresponse

          # Set up logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")

          def lambda_handler(event, context):
            logger.info("Inside function to add S3 Trigger")
            logger.info("Received event: " + json.dumps(event, indent=2))

            source_bucket_name = event['ResourceProperties']["SourceBucketName"]
            target_lambda_arn = event['ResourceProperties']['TargetLambdaArn']
            notifications_save_mode = event['ResourceProperties']['NotificationsSaveMode']
            prefix_pattern = event['ResourceProperties']['SparkLambdaTriggerFilePattern']
            
            try:
              lambda_configurations = []
              if notifications_save_mode.lower() == "insert":
                lambda_configurations.append({'LambdaFunctionArn': target_lambda_arn, 'Events': ['s3:ObjectCreated:*']})
              elif notifications_save_mode.lower() == "append":
                s3_client = boto3.client('s3')
                existing_configurations = s3_client.get_bucket_notification_configuration(Bucket=source_bucket_name)
                for config in existing_configurations['LambdaFunctionConfigurations']:
                  config_dict = {}
                  config_dict['LambdaFunctionArn'] = config['LambdaFunctionArn']
                  config_dict['Events'] = config['Events']
                  config_dict['Filter'] = config['Filter']
                  lambda_configurations.append(config_dict)
                lambda_configurations.append({'LambdaFunctionArn': target_lambda_arn, 'Events': ['s3:ObjectCreated:*'], 'Filter': {'Key': {'FilterRules': [{'Name': 'Prefix', 'Value': f'{prefix_pattern}'}]}}})
              else:
                raise Exception("Notifications save mode is invalid. Permissible values: Append/Insert.")
              logger.info(f"Printing S3 Trigger config: {lambda_configurations}")
              s3 = boto3.resource('s3')  
              bucket_notification = s3.BucketNotification(source_bucket_name)
              response = bucket_notification.put(
                NotificationConfiguration={
                  'LambdaFunctionConfigurations': lambda_configurations
              })
              responseData={'source_bucket_name':source_bucket_name}
              responseStatus = 'SUCCESS'
              logger.info(f"S3 Trigger successfully added for {source_bucket_name}")
            except Exception as e:
              responseStatus = 'FAILED'
              responseData = {'Failure': 'Something bad happened.'}
              logger.info(f"S3 Trigger failed for {source_bucket_name}: {e}")
              raise e
            cfnresponse.send(event, context, responseStatus, responseData, "CustomResourcePhysicalID")
      Runtime: python3.10
      Description: A function that merges all the file names from S3 tmp files.
      Role: !GetAtt SparkLambdaS3TriggerLambdaRole.Arn
      Timeout: 50

  FilesMergerInvokeS3TriggerLambda:
    Type: 'Custom::InvokeFilesMergerS3TriggerLambda'
    DependsOn: FilesMergerLambdaInvokePermission
    Properties:
      ServiceToken: !GetAtt SparkLambdaS3TriggerLambda.Arn
      TargetLambdaArn: !GetAtt SparkLambda.Arn
      SourceBucketName: !Ref FilesMergerSourceBucketName
      NotificationsSaveMode: !Ref SparkLambdaNotificationsSaveMode
      SparkLambdaTriggerFilePattern: !Sub '${FilesCollectorMetadataKeyName}/${SparkLambdaTriggerFile}'