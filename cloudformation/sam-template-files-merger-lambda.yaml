AWSTemplateFormatVersion: '2010-09-09'
Transform: 'AWS::Serverless-2016-10-31'

Parameters:
  LambdaFunctionPrefix:
    Description: 'Optional: This is the prefix for the name of the lambda function that will be created. This name must satisfy the pattern ^[a-z0-9-_]{1,64}$'
    Type: String
    Default: FilesMerger
  LambdaTimeout:
    Description: 'Optional: Maximum Lambda invocation runtime in seconds. (min 1 - 900 max)'
    Default: 120
    Type: Number
  LambdaMemory:
    Description: 'Optional: Lambda memory in MB (min 128 - 3008 max).'
    Default: 128
    Type: Number
  SleepTime:
    Type: Number
    Description: 'Mandatory: Sleep time in seconds to recursively check for flag file e.g. 5'
  SourceBucketName:
    Type: String
    Description: 'Mandatory: Source bucket where unprocessed_file will be placed e.g. bb-datalake'
  NotificationsSaveMode:
    Type: String
    Description: 'Optional: Notifications can be inserted or appended So only two values allowed: Insert/Append'
    Default: Append
  
Resources:
  FilesMergerLambda:
    Type: 'AWS::Serverless::Function'
    Properties:
      FunctionName: !Sub '${LambdaFunctionPrefix}-${AWS::StackName}'
      Handler: index.lambda_handler
      InlineCode: |
          import json
          import boto3
          import botocore
          import logging
          import time

          # Set up logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")

          def lambda_handler(event, context):
            bucket_name = event["Records"][0]["s3"]["bucket"]["name"]
            object_key = event["Records"][0]["s3"]["object"]["key"]
            flag_file = object_key.replace("_tmp","_flag") 
    
            sleep_time_secs = os.environ['Sleep_Time']

            add_delay_to_execution(bucket_name,flag_file)
            merge_files_content(bucket_name,object_key)

          def add_delay_to_execution(s3_bucket_script: str,object_key: str) -> None:
            try:
              logger.info('Inside function add_delay_to_execution')
              s3 = boto3.resource("s3")
              last_modified_date = s3.Object(s3_bucket_script, object_key).last_modified
              current_modified_date = ""
              i = 1
              while current_modified_date != last_modified_date:
                time.sleep(sleep_time_secs)
                current_modified_date = last_modified_date
                last_modified_date = s3.Object(s3_bucket_script, object_key).last_modified
                logger.info(f'Iteration : {i}, last_modified_date: {last_modified_date}, current_modified_date: {current_modified_date}')
                i = i + 1
            except botocore.exceptions.ClientError as e:
              logger.error(f"Boto Error: {e.response['Error']['Code']}")
            except Exception as e :
              logger.error(f"Error: {e}")
            else:
              logger.info('Successfully added delay to the execution')

          def merge_files_content(bucket_name: str,object_key: str) -> None:
            try:
              logger.info('Inside function merge_files_content')
              s3 = boto3.resource("s3")
              required_objects = s3.Bucket(bucket_name).objects.filter(Prefix=object_key.replace("unprocessed_tmp",""))
              content = ""
              try:
                logger.info(f'Reading {object_key.replace("_tmp","_file")} if it exists')
                content = s3.Object(bucket_name,object_key.replace("_tmp","_file")).get()['Body'].read().decode('utf-8') + "\n"
                logger.info(f'{object_key.replace("_tmp","_file")} exists, Content: {content}')
              except botocore.exceptions.ClientError as e:
	              logger.info(f'{object_key.replace("_tmp","_file")} does not exist')
              for object in required_objects:
                if "tmp" in object.key:
                  logger.info(f'Merging file: {object.key}')
                  content = content + object.get()['Body'].read().decode('utf-8') + "\n"
              content = content.strip()
              logger.info(f'Merged content: {content}')
              logger.info(f'Writing merged content to file: {object_key.replace("_tmp","_file")}')
              s3.Bucket(bucket_name).put_object(Key=object_key.replace("_tmp","_file"), Body=content.encode('utf-8'))
              logger.info('Deleting tmp files')
              for object in required_objects:
                if "tmp" in object.key:
                  logger.info(f'Deleting file: {object.key}')
                  object.delete()
              logger.info(f'Deleting file: {object_key.replace("_tmp","_flag")}')
              s3.Object(bucket_name, object_key.replace("_tmp","_flag")).delete()
            except botocore.exceptions.ClientError as e:
              logger.error(f"Boto Error: {e.response['Error']['Code']}")
            except Exception as e :
              logger.error(f"Error: {e}")
            else:
              logger.info(f'Successfully merged files content and written to {object_key.replace("_tmp","_file")}')
      Runtime: python3.10
      Description: A function that merges all the files containing s3 file names into a single file(unprocessed_file).
      Role: !Sub "arn:aws:iam::${AWS::AccountId}:role/service-role/datalake-lambda-s3-permission"
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      ReservedConcurrentExecutions: 1
      Environment:
        Variables:
          Sleep_Time: !Ref SleepTime
  
  FilesMergerLambdaInvokeConfig:
    Type: 'AWS::Lambda::EventInvokeConfig'
    DependsOn: FilesMergerLambda
    Properties:
      FunctionName: !Ref FilesMergerLambda
      MaximumRetryAttempts: 0
      Qualifier: "$LATEST"

  LambdaInvokePermission:
    Type: 'AWS::Lambda::Permission'
    Properties:
      FunctionName: !GetAtt FilesMergerLambda.Arn
      Action: 'lambda:InvokeFunction'
      Principal: s3.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
      SourceArn: !Sub 'arn:aws:s3:::${SourceBucketName}'

  S3TriggerLambdaRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Path: /
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetBucketNotification'
                  - 's3:PutBucketNotification'
                Resource: !Sub 'arn:aws:s3:::${SourceBucketName}'
              - Effect: Allow
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: 'arn:aws:logs:*:*:*'
  
  S3TriggerLambda:
    Type: 'AWS::Lambda::Function'
    DependsOn: SparkLambdaInvokeConfig
    Properties:
      FunctionName: !Sub 'S3TriggerLambda-${AWS::StackName}'
      Handler: index.lambda_handler
      Code:
        ZipFile: |

          import json
          import logging
          import boto3
          import cfnresponse

          # Set up logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")

          def lambda_handler(event, context):
            logger.info("Inside function to add S3 Trigger")
            logger.info("Received event: " + json.dumps(event, indent=2))

            source_bucket_name = event['ResourceProperties']["SourceBucketName"]
            target_lambda_arn = event['ResourceProperties']['TargetLambdaArn']
            notifications_save_mode = event['ResourceProperties']['NotificationsSaveMode']
            
            try:
              lambda_configurations = []
              if notifications_save_mode.lower() == "insert":
                lambda_configurations.append({'LambdaFunctionArn': target_lambda_arn, 'Events': ['s3:ObjectCreated:*']})
              elif notifications_save_mode.lower() == "append":
                s3_client = boto3.client('s3')
                existing_configurations = s3_client.get_bucket_notification_configuration(Bucket=source_bucket_name)
                for config in existing_configurations['LambdaFunctionConfigurations']:
                  config_dict = {}
                  config_dict['LambdaFunctionArn'] = config['LambdaFunctionArn']
                  config_dict['Events'] = config['Events']
                  lambda_configurations.append(config_dict)
                lambda_configurations.append({'LambdaFunctionArn': target_lambda_arn, 'Events': ['s3:ObjectCreated:*']})
              else:
                raise Exception("Notifications save mode is invalid. Permissible values: Append/Insert.")
              logger.info(f"Printing S3 Trigger config: {lambda_configurations}")
              s3 = boto3.resource('s3')  
              bucket_notification = s3.BucketNotification(source_bucket_name)
              response = bucket_notification.put(
                NotificationConfiguration={
                  'LambdaFunctionConfigurations': lambda_configurations
              })
              responseData={'source_bucket_name':source_bucket_name}
              responseStatus = 'SUCCESS'
              logger.info(f"S3 Trigger successfully added for {source_bucket_name}")
            except Exception as e:
              responseStatus = 'FAILED'
              responseData = {'Failure': 'Something bad happened.'}
              logger.info(f"S3 Trigger failed for {source_bucket_name}: {e}")
              raise e
            cfnresponse.send(event, context, responseStatus, responseData, "CustomResourcePhysicalID")
      Runtime: python3.10
      Description: A function that merges all the file names from S3 tmp files.
      Role: !GetAtt S3TriggerLambdaRole.Arn
      Timeout: 50

  InvokeS3TriggerLambda:
    Type: 'Custom::InvokeS3TriggerLambda'
    DependsOn: LambdaInvokePermission
    Properties:
      ServiceToken: !GetAtt S3TriggerLambda.Arn
      TargetLambdaArn: !GetAtt FilesMergerLambda.Arn
      SourceBucketName: !Ref SourceBucketName
      NotificationsSaveMode: !Ref NotificationsSaveMode