AWSTemplateFormatVersion: '2010-09-09'
Transform: 'AWS::Serverless-2016-10-31'

Parameters:
  FilesCollectorLambdaFunctionPrefix:
    Description: 'Optional: This is the prefix for the name of the lambda function that will be created. This name must satisfy the pattern ^[a-z0-9-_]{1,64}$'
    Type: String
    Default: FilesCollector
  FilesCollectorLambdaTimeout:
    Description: 'Optional: Maximum Lambda invocation runtime in seconds. (min 1 - 900 max)'
    Default: 60
    Type: Number
  FilesCollectorLambdaMemory:
    Description: 'Optional: Lambda memory in MB (min 128 - 3008 max).'
    Default: 128
    Type: Number
  FilesCollectorMetadataBucketName:
    Type: String
    Description: 'Mandatory: Bucket where the metadata will be persisted e.g. bb-datalake'
  FilesCollectorMetadataKeyName:
    Type: String
    Description: 'Mandatory: Key where the metadata will be persisted e.g. metadata/bb_market_delta'
  FilesMergerLambdaFunctionPrefix:
    Description: 'Optional: This is the prefix for the name of the lambda function that will be created. This name must satisfy the pattern ^[a-z0-9-_]{1,64}$'
    Type: String
    Default: FilesMerger
  FilesMergerLambdaTimeout:
    Description: 'Optional: Maximum Lambda invocation runtime in seconds. (min 1 - 900 max)'
    Default: 120
    Type: Number
  FilesMergerLambdaMemory:
    Description: 'Optional: Lambda memory in MB (min 128 - 3008 max).'
    Default: 128
    Type: Number
  FilesMergerSleepTime:
    Type: Number
    Description: 'Mandatory: Sleep time in seconds to recursively check for flag file e.g. 5'

Resources:
  FilesCollectorLambda:
    Type: 'AWS::Serverless::Function'
    Properties:
      FunctionName: !Sub '${FilesCollectorLambdaFunctionPrefix}-${AWS::StackName}'
      Handler: index.lambda_handler
      InlineCode: |
          import json
          import logging
          import boto3
          import botocore
          import os

          # Set up logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")

          #Get environment variables
          metadata_bucket_name = os.environ["Metadata_Bucket_Name"]
          metadata_key_name = os.environ["Metadata_Key_Name"]

          def lambda_handler(event, context):
            s3_object = getS3Object(event)
            writeToS3(s3_object)

          def getS3Object(event):
            logger.info("Inside function getS3Object")
            bucket_name = event["Records"][0]["s3"]["bucket"]["name"]
            object_key = event["Records"][0]["s3"]["object"]["key"]
    
            s3_object = "s3a://" + bucket_name + "/" + object_key
            logger.info(f"S3 Object Name: {s3_object}")
    
            return s3_object

          def writeToS3(s3_object):
            logger.info("Inside function writeToS3")
            encoded_string = s3_object.encode("utf-8")
        
            file_name = "unprocessed"
            s3_path = metadata_key_name + "/" + file_name + "_tmp"
            s3_path_flag = metadata_key_name + "/" + file_name + "_flag"

            s3 = boto3.resource("s3")
        
            try:
              body_flag_file = s3.Object(metadata_bucket_name, s3_path_flag).get()['Body'].read().decode('utf-8')
              if int(body_flag_file) == 0:
                read_content_and_push_file(metadata_bucket_name, s3_path, encoded_string)
                flag_int = int(body_flag_file) + 1
                flag_string = str(flag_int)
                logger.info(f"Content successfully written to file {s3_path}")
              else:
                new_file_name = (s3_path.replace("_tmp","")).replace(file_name,"")+f"tmp_{body_flag_file}_{file_name}"
                read_content_and_push_file(metadata_bucket_name, new_file_name, encoded_string)
                flag_int = int(body_flag_file) + 1
                if flag_int > 10:
                  flag_string = "0"
                else:
                  flag_string = str(flag_int)
                logger.info(f"Content successfully written to file {new_file_name}")
              s3.Bucket(metadata_bucket_name).put_object(Key=s3_path_flag, Body=flag_string.encode("utf-8"))
              logger.info(f"Flag successfully written to file {s3_path_flag}")
            except botocore.exceptions.ClientError as e:
              if e.response["Error"]["Code"] == "NoSuchKey":
                s3.Bucket(metadata_bucket_name).put_object(Key=s3_path, Body=encoded_string)
                s3.Bucket(metadata_bucket_name).put_object(Key=s3_path_flag, Body="1".encode("utf-8"))
                logger.info(f"Content successfully written to file {s3_path}")
                logger.info(f"Flag successfully written to file {s3_path_flag}")
              else:
                logger.info(f"Error: {e.response['Error']['Code']}")

          def read_content_and_push_file(bucket_name, file_name, new_content) -> None:
            s3 = boto3.resource("s3")
            try:
              content = s3.Object(bucket_name, file_name).get()['Body'].read().decode('utf-8')
              content = content.strip() + "\n" + new_content.decode('utf-8')
              encoded_content = content.encode('utf-8')
              s3.Bucket(bucket_name).put_object(Key=file_name, Body=encoded_content)
            except Exception as e :
              s3.Bucket(bucket_name).put_object(Key=file_name, Body=new_content)
      Runtime: python3.10
      Description: A function that collects all the file names from S3Trigger.
      Role: !Sub "arn:aws:iam::${AWS::AccountId}:role/service-role/datalake-lambda-s3-permission"
      Timeout: !Ref FilesCollectorLambdaTimeout
      MemorySize: !Ref FilesCollectorLambdaMemory
      ReservedConcurrentExecutions: 1
      Environment:
        Variables:
          Metadata_Bucket_Name: !Ref FilesCollectorMetadataBucketName
          Metadata_Key_Name: !Ref FilesCollectorMetadataKeyName
  
  FilesCollectorLambdaInvokeConfig:
    Type: 'AWS::Lambda::EventInvokeConfig'
    DependsOn: FilesCollectorLambda
    Properties:
      FunctionName: !Ref FilesCollectorLambda
      MaximumRetryAttempts: 0
      Qualifier: "$LATEST"

  FilesMergerLambda:
    Type: 'AWS::Serverless::Function'
    Properties:
      FunctionName: !Sub '${FilesMergerLambdaFunctionPrefix}-${AWS::StackName}'
      Handler: index.lambda_handler
      InlineCode: |
          import json
          import boto3
          import botocore
          import logging
          import time
          import os

          # Set up logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")

          def lambda_handler(event, context):
            bucket_name = event["Records"][0]["s3"]["bucket"]["name"]
            object_key = event["Records"][0]["s3"]["object"]["key"]
            flag_file = object_key.replace("_tmp","_flag") 
    
            sleep_time_secs = os.environ['Sleep_Time']

            add_delay_to_execution(bucket_name,flag_file)
            merge_files_content(bucket_name,object_key)

          def add_delay_to_execution(s3_bucket_script: str,object_key: str) -> None:
            try:
              logger.info('Inside function add_delay_to_execution')
              s3 = boto3.resource("s3")
              last_modified_date = s3.Object(s3_bucket_script, object_key).last_modified
              current_modified_date = ""
              i = 1
              while current_modified_date != last_modified_date:
                time.sleep(sleep_time_secs)
                current_modified_date = last_modified_date
                last_modified_date = s3.Object(s3_bucket_script, object_key).last_modified
                logger.info(f'Iteration : {i}, last_modified_date: {last_modified_date}, current_modified_date: {current_modified_date}')
                i = i + 1
            except botocore.exceptions.ClientError as e:
              logger.error(f"Boto Error: {e.response['Error']['Code']}")
            except Exception as e :
              logger.error(f"Error: {e}")
            else:
              logger.info('Successfully added delay to the execution')

          def merge_files_content(bucket_name: str,object_key: str) -> None:
            try:
              logger.info('Inside function merge_files_content')
              s3 = boto3.resource("s3")
              required_objects = s3.Bucket(bucket_name).objects.filter(Prefix=object_key.replace("unprocessed_tmp",""))
              content = ""
              try:
                logger.info(f'Reading {object_key.replace("_tmp","_file")} if it exists')
                content = s3.Object(bucket_name,object_key.replace("_tmp","_file")).get()['Body'].read().decode('utf-8') + "\n"
                logger.info(f'{object_key.replace("_tmp","_file")} exists, Content: {content}')
              except botocore.exceptions.ClientError as e:
                logger.info(f'{object_key.replace("_tmp","_file")} does not exist')
              for object in required_objects:
                if "tmp" in object.key:
                  logger.info(f'Merging file: {object.key}')
                  content = content + object.get()['Body'].read().decode('utf-8') + "\n"
              content = content.strip()
              logger.info(f'Merged content: {content}')
              logger.info(f'Writing merged content to file: {object_key.replace("_tmp","_file")}')
              s3.Bucket(bucket_name).put_object(Key=object_key.replace("_tmp","_file"), Body=content.encode('utf-8'))
              logger.info('Deleting tmp files')
              for object in required_objects:
                if "tmp" in object.key:
                  logger.info(f'Deleting file: {object.key}')
                  object.delete()
              logger.info(f'Deleting file: {object_key.replace("_tmp","_flag")}')
              s3.Object(bucket_name, object_key.replace("_tmp","_flag")).delete()
            except botocore.exceptions.ClientError as e:
              logger.error(f"Boto Error: {e.response['Error']['Code']}")
            except Exception as e :
              logger.error(f"Error: {e}")
            else:
              logger.info(f'Successfully merged files content and written to {object_key.replace("_tmp","_file")}')
      Runtime: python3.10
      Description: A function that merges all the files containing s3 file names into a single file(unprocessed_file).
      Role: !Sub "arn:aws:iam::${AWS::AccountId}:role/service-role/datalake-lambda-s3-permission"
      Timeout: !Ref FilesMergerLambdaTimeout
      MemorySize: !Ref FilesMergerLambdaMemory
      ReservedConcurrentExecutions: 1
      Environment:
        Variables:
          Sleep_Time: !Ref FilesMergerSleepTime
  
  FilesMergerLambdaInvokeConfig:
    Type: 'AWS::Lambda::EventInvokeConfig'
    DependsOn: FilesMergerLambda
    Properties:
      FunctionName: !Ref FilesMergerLambda
      MaximumRetryAttempts: 0
      Qualifier: "$LATEST"